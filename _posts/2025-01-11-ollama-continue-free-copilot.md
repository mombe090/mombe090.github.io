---
title: "Coder avec l'IA : Ollama, Continue et DeepSeek Coder v2, les Nouvelles R√©volutions du Pair Programming"
description: "Ces outils ne rivalisent pas seulement avec des solutions comme GitHub Copilot pomper √† bloc avec GPT-4o ou autres models propri√©taires; ils sont peut √™tre mieux"
categories: [ai, development]
tags: [ollama, copilot, free, llm]
author: "mombe090"
image:
  path: /assets/img/header/ollama-continue-free-copilot.webp
---

## Contexte :
Nous vivons certainement l'une des p√©riodes les plus int√©ressentes dans l'informatique depuis l'av√®nement de l'internet. <br />

Les mod√®les LLM (Large Langage Model) sont de plus en plus √©volu√©s, capables de comprendre et de g√©n√©rer du contenu textuel, image, audio, vid√©o de mani√®re coh√©rente et convaincante. <br />
Ces mod√®les sont entrain√©s et compr√©ss√©s avec beaucoup de donn√©es (pr√®s que toute la connaissance humaine num√©ris√©e). <br />

C'est un grand changement dans notre mani√®re d'interagir et utiliser les ordinateurs, telephones et autres objets connect√©s. <br />

Tout √ßa pour dire que √ßa va changer beaucoup de choses dans notre quotidien et dans notre mani√®re de travailler, elle remplacera beaucoup de m√©tiers, mais en cr√©era aussi beaucoup d'autre comme √ßa √©t√© le cas de la machine √† Vapeur, l'√©lectricit√©, l'internet ainsi que d'autres inventions humaines √† travers son √©volution üåé <br />

Dans le milieu du d√©veloppement de logiciels, on peut s'en servir comme pair-programmeur **(co√©quipier comme au Tenis √† 2)**, mentor, coach, ... <br />

[Open-AI](https://openai.com/) a √©t√© le pionnier en sortant leur model **GPT-3.5** en Novembren 2022, plusieurs autres mod√®les gratuits comme payants ont suivi. <br />
`Microsoft` en collaboration avec `OpenAI` a sorti `Github Copilot` est un outil de **pair-programming** qui vous aide dans vos t√¢ches de development √† l'interieur de votre IDE (vscode, jetbrains ...). 

Pendant une ann√©e durant sa *beta teste*, elle √©tait gratuite pour tout le monde, rendu payante [10$ USD/mois](https://github.com/features/copilot/plans?cft=copilot_li.features_copilot). <br />

## Objectif : 
Sur cet article, nous allons voir comment ex√©cuter quelques models open-source en local avec le [Ollama](https://ollama.ai/) et comment utiliser [continue](https://github.com/continuedev/continue) pour servir de pair-programmeur dans un IDE de votre choix (vscode ou jetbrains idea ...).

## C'est quoi un LLM ?
Un grand mod√®le de langage (LLM pour Large Language Model) est un mod√®le d‚Äôapprentissage automatique capable de comprendre et g√©n√©rer des textes.
Ils op√®rent en analysant des volumes massifs de donn√©es de langage.
***source [cloudflare](https://www.cloudflare.com/fr-fr/learning/ai/what-is-large-language-model/)***

### C'est quoi Ollama ?
[Ollama](https://ollama.ai/) est un est projet open-source qui permet d'ex√©cuter les models de langage de LLM (Large Langage Model) en local. 
Il supporte beaucoup de models comme `Llama`, `Mistral`, `Gemma`, `DeepSeek`, `Phi` ... voir la liste compl√®te [ici](https://ollama.ai/library). 

Tr√®s simple √† installer et disponible sur le [docker hub](https://hub.docker.com/r/ollama/ollama) ainsi que sur les trois principaux OS (Windows, MacOS et Linux).

### C'est quoi Continue ?
[Continue](https://github.com/continuedev/continue) est plugin ou extension pour IDE qui s'interface avec `Ollama` ou tout autres agents du genre pour fournir des fonctionnalit√©s d'assistant ou copilot dans votre environnement de d√©veloppement. <br />
Il est disponible pour [VSCode](https://marketplace.visualstudio.com/items?itemName=Continue.continue) et [JetBrains IDE](https://plugins.jetbrains.com/plugin/22707-continue). <br />

> Je lui ai d√©couvert sur cet [podcast](https://podcasts.apple.com/us/podcast/devops-and-docker-talk-cloud-native-interviews-and-tooling/id1451860877) de `Bret Fisher` avec comme invit√© le cocr√©ateur de `Continue` [voir la version vid√©o avec d√©mo ici](https://www.youtube.com/watch?v=9zyJrMwGEKs)
{: .prompt-info }

Dans cet article, nous allons voir l'int√©gration `Continue` avec deux models open-source (local via `Ollama` et disponible sur internet) entrain√© sp√©cialement pour g√©n√©rer du code :

C'est mod√®les sp√©cialis√©s en g√©n√©ration de code et d'assistance aux d√©veloppements donc plus l√©g√®re que leurs homologues g√©n√©ralistes.

Chaque mod√®le se pr√©sente avec diff√©rents tags pour le nombre de param√®tres sur lequel il a √©t√© entrain√© et plus celui-ci est gros plus, plus sa taille est grande et plus il est performant.
> Choisir celui qui correspond avec les ressources de votre machine ou la rapidit√© de votre internet.
{: .prompt-warning }

- [Codestral](https://mistral.ai/news/codestral/) un mod√®le open-source de la soci√©t√© fran√ßaise Mistral AI.
  - Ce mod√®le est particuli√®rement bon pour mes activit√©s de coding perso, de devops et homelab
  - Il n'est pas en local par contre, il est h√©berg√© quelque part dans les serveurs de Mistral AI
  - Il est sous la nouvelle licence [Mistral AI Non-Production License](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/), gratuit pour des travaux d'apprentissages et de recherche.
  - Cr√©er un compte sur [Mistral](https://mistral.ai) puis rechercher codestral pour r√©cuperer le token d'authentification, voir son [int√©gration avec continue ici](https://docs.mistral.ai/capabilities/code_generation/#how-to-set-up-codestral-with-continue) : 
- [DeepSeek Coder V2](https://ollama.com/library/deepseek-coder-v2) un mod√®le open-source de la soci√©t√© chinoise DeepSeek .
  - Ce mod√®le est tr√®s bon pour g√©n√©rer du code, mais il est tr√®s gourmand en ressources.
  - La plus l√©g√®re version 16b p√®se 8.9GB
  - **Disponible sur `Ollama`**

- [Qwen2.5-coder](https://ollama.com/library/qwen2.5-coder), 
  - si vous √™tes contraint par les resources disponible, internet ou m√™me mat√©rielles, celui-ci peut-√™tre un bon model pour tester l'utilisation d'`Ollama` et `Continue`
  - Son mod√®le de 1.5B p√®se moins de 1GB
  - **Disponible sur `Ollama`**

### Installation

Vous pouvez installer `ollama` en utilisant des outils de conteneurisation comme `docker` / `podman` ou l'installer directement sur votre syst√®me d'exploitation.
- Docker ou Podman : voir les instructions [ici](https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image)
- Syst√®me d'exploitation : T√©l√©charger [ici](https://ollama.com/download)

Pour installer `continue` en fonction que vous soyez sur vscode ou les IDE de jetbrains :
- `vscode` : ouvrez l'appliction -> puis `Extensions` -> rechercher `Continue` -> `Install`
  - ![](../assets/img/content/continue.png)
- `Intellij` : 
  - `Intellij Idea` -> puis `Settings` -> puis `Pluggins` -> rechercher `Continue` -> `Install`
  - ![](../assets/img/content/continue-idea.png)

### Utilisation :

Dans cet exemple, j'ai opt√© pour la s√©conde, si vous l'utiliser quotidiennement comme moi, assurez-vous q'`Ollama`se lance au d√©marrage de votre syst√®me.
Quand `Ollama` est d√©marr√©, vous verrez sur votre barre des t√¢ches une ic√¥ne repr√©sentation l'animal **Lama**.

#### Teste avec petit mod√®le :
Nous allons tester que tout est fonctionnel avec cet model `qwen2.5-coder:1.5b` <1024MB :
```bash
ollama run qwen2.5-coder:1.5b
pulling manifest
pulling 29d8c98fa6b0... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 986 MB
pulling 66b9ea09bd5b... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   68 B
pulling e94a8ecb9327... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.6 KB
pulling 832dd9e00a68... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  11 KB
pulling 152cb442202b... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  487 B
verifying sha256 digest
writing manifest
success
>>> Qui est Linus Torvalds ?
Linus Torvalds est un informaticien et chef de projet fondamental de la Linux, l'une des versions les plus importantes de la famille d'operating syst√®mes Unix et GNU/Linux.
Il acontribu√© significativement √† la conception, la programmation et la diffusion de Linux. 
Linus Torvalds est connu pour son travail sur le d√©veloppement de l'architecture Linux et pour ses contributions cruciales au syst√®me d'exploitation Linux.
```
Quand vous avez une sortie pareille √† votre prompt, √ßa veut dire que vous √™tes pr√™t pour la suite, `ctrl/cmd + D` pour sortir. 

Pour apprendre plus sur la command `ollam`, execute `ollama help`:
```shell
ollama help

Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.

```
### Configurer continue avec deepSeek-coder-v2 :
Continue g√©n√®re un dossier `.continue` dans le dossier d'utilisateur [plus de d√©tail ici](https://docs.continue.dev/customize/overview), ouvrer le dossier puis √©diter le fichier `~/.continue/config.json`.

Ci-dessous, j'utilise ici le model de `deePseek-coder-v2` avec 16 Milliards de param√®tres, si vous optez pour un autre mod√®le plus l√©ger adapter la config.

```json
{
  "models": [
    {
      "title": "DeepSeek Coder V2",
      "model": "deepseek-coder-v2",
      "provider": "ollama"
    }
  ],
  "tabAutocompleteModel": {
     "title": "DeepSeek Coder V2",
     "model": "deepseek-coder-v2",
     "provider": "ollama"
  }
}
```
Enregistrer les et retournez dans votre IDE :
- `vscode`: Cliquer sur l'ic√¥ne de continue parmi les extensions install√©es 
  - ![](../assets/img/content/deepseek-v2-vscode.png)
  - Ouvrez un de vos projets et commenc√© √† coder, avec la section `Tab Autocomplete Model` configurer dans `~/.continue/config.json`, vous devriez voir une suggestion automatique comme sur l'image ci-dessous :
  - ![](../assets/img/content/continue-autocomplete.png)

## Les alternatives :
- *Continue* : Il existe une alternative avec laquel on peut utiliser codestral de mani√®re gratuite qui s'appel [Tabnine](https://www.tabnine.com/)
- *Ollalma* : l'une des alternative est [Lm Studio](https://lmstudio.ai/docs/api/server)


## R√©ferences :
- [Ollama](https://github.com/ollama/ollama)
- [Continue](https://docs.continue.dev/)
- [Mistral](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/)
